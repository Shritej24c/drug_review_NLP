{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1eccbbb",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trax in /Applications/anaconda3/lib/python3.8/site-packages (1.4.1)\r\n",
      "Requirement already satisfied: absl-py in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (1.4.0)\r\n",
      "Requirement already satisfied: funcsigs in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (1.0.2)\r\n",
      "Requirement already satisfied: gin-config in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (0.5.0)\r\n",
      "Requirement already satisfied: gym in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (0.21.0)\r\n",
      "Requirement already satisfied: jax in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (0.4.13)\r\n",
      "Requirement already satisfied: jaxlib in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (0.4.13)\r\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (3.7.2)\r\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (1.22.4)\r\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (5.8.0)\r\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (1.7.0)\r\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (1.15.0)\r\n",
      "Requirement already satisfied: tensorflow-datasets in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (4.9.2)\r\n",
      "Requirement already satisfied: tensorflow-text in /Applications/anaconda3/lib/python3.8/site-packages (from trax) (2.13.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Applications/anaconda3/lib/python3.8/site-packages (from gym->trax) (1.6.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from jax->trax) (0.2.0)\r\n",
      "Requirement already satisfied: opt-einsum in /Applications/anaconda3/lib/python3.8/site-packages (from jax->trax) (3.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from jax->trax) (6.1.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (0.10.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (4.42.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (1.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (23.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (2.8.1)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->trax) (5.4.0)\r\n",
      "Requirement already satisfied: array-record in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (0.4.0)\r\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (7.1.2)\r\n",
      "Requirement already satisfied: dm-tree in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (0.1.8)\r\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.3.0)\r\n",
      "Requirement already satisfied: promise in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (2.3)\r\n",
      "Requirement already satisfied: protobuf>=3.20 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (3.20.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (2.25.1)\r\n",
      "Requirement already satisfied: tensorflow-metadata in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.14.0)\r\n",
      "Requirement already satisfied: termcolor in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.1.0)\r\n",
      "Requirement already satisfied: toml in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (0.10.2)\r\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (4.59.0)\r\n",
      "Requirement already satisfied: wrapt in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.12.1)\r\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-text->trax) (0.14.0)\r\n",
      "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-text->trax) (2.13.0)\r\n",
      "Requirement already satisfied: typing_extensions in /Applications/anaconda3/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp in /Applications/anaconda3/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (3.4.1)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.26.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (23.5.26)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (3.1.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (16.0.0)\r\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (52.0.0.post20210125)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.53.0)\r\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.13.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.13.0)\r\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.13.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.32.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.60.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.36.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (2.22.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (3.4.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.7.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.0.1)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (4.2.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (4.7.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->trax) (3.1.1)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad32dc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9610389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Applications/anaconda3/lib/python3.8/site-packages (1.8.0)\r\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Applications/anaconda3/lib/python3.8/site-packages (from wordcloud) (1.22.4)\r\n",
      "Requirement already satisfied: pillow in /Applications/anaconda3/lib/python3.8/site-packages (from wordcloud) (9.5.0)\r\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.8/site-packages (from wordcloud) (3.7.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.42.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (23.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib->wordcloud) (5.4.0)\r\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.4.1)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: Pillow==9.5.0 in /Applications/anaconda3/lib/python3.8/site-packages (9.5.0)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: pip in /Applications/anaconda3/lib/python3.8/site-packages (23.2.1)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: textblob in /Applications/anaconda3/lib/python3.8/site-packages (0.17.1)\r\n",
      "Requirement already satisfied: nltk>=3.1 in /Applications/anaconda3/lib/python3.8/site-packages (from textblob) (3.6.1)\r\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.3.2)\r\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2021.4.4)\r\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.59.0)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install Pillow==9.5.0\n",
    "!pip install --upgrade pip\n",
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b480f62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Applications/anaconda3/lib/python3.8/site-packages (3.6.1)\r\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\r\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e52c2395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06eb5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /Applications/anaconda3/lib/python3.8/site-packages (0.22.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.7.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.22.4)\r\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.2.4)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.3.0)\r\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (3.7.2)\r\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.3.2)\r\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.8/site-packages (from mlxtend) (52.0.0.post20210125)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (4.42.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (23.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Applications/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (5.4.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Applications/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.2->mlxtend) (2021.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from scikit-learn>=1.0.2->mlxtend) (2.1.0)\r\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.0->mlxtend) (3.4.1)\r\n",
      "\u001B[33mDEPRECATION: arcgis 1.9.1 has a non-standard dependency specifier keyring<=21.8.*,>=19. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of arcgis or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: celery 5.1.0 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec934cd",
   "metadata": {},
   "source": [
    "### 1. Installing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aff11",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "import re\n",
    "#import trax\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6e95c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0eb6b7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3e32b",
   "metadata": {},
   "source": [
    "### 2. Importing CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ebe71",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"/Users/shritej/Documents/GitHub/archive/drugsComTest_raw.csv\")\n",
    "df_train = pd.read_csv(\"/Users/shritej/Documents/GitHub/archive/drugsComTrain_raw.csv\")\n",
    "\n",
    "print (\"The shape of the train set given is : \", df_train.shape)\n",
    "print (\"The shape of the test set given is : \", df_test.shape)\n",
    "\n",
    "df_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d81e79",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e51c8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74894cd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data=pd.concat([df_train,df_test])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ed0a3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089c990",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f8816",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d8f91",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83313a51",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01130496",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Null Value\n",
    "print (\"Null values in the dataset : \", data.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6f38f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculating what percentage of data is null\n",
    "size = data.shape[0]\n",
    "\n",
    "print (\"Total Size of the dataset : \", size)\n",
    "\n",
    "total_na = data.isnull().sum(axis = 0)['condition']\n",
    "print (\"Null values : \", total_na)\n",
    "\n",
    "print (\"PERCENTAGE : \", (total_na/size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6389115",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = data.dropna(how = 'any', axis = 0)\n",
    "\n",
    "print (\"The shape of the dataset after null values removal :\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d0584",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8e537",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.sort_values(['uniqueid'], ascending = True, inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbde908",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print (data['condition'].nunique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b0268",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print (\"some of the conditions are : \", data['condition'].unique()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d3560",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.loc[data['rating'] == 1, :]['drugname'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe68cef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.loc[data.usefulcount == 0, 'drugname'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fd1a5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data['rating'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4718201",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2275516",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe415b6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#This barplot shows the top 20 drugs with the 10/10 rating\n",
    "sns.set(font_scale = 1.2, style = 'darkgrid')\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "rating = dict(data.loc[data.rating == 10, \"drugname\"].value_counts())\n",
    "drugname = list(rating.keys())\n",
    "drug_rating = list(rating.values())\n",
    "\n",
    "sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20])\n",
    "\n",
    "sns_rating.set(title = 'Top 20 drugs with 10/10 rating', ylabel = 'Number of Ratings', xlabel = \"Drug Names\")\n",
    "plt.setp(sns_rating.get_xticklabels(), rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd388c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "rating = dict(data.loc[data.rating == 1, \"drugname\"].value_counts())\n",
    "drugname = list(rating.keys())\n",
    "drug_rating = list(rating.values())\n",
    "\n",
    "sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20], palette = 'winter')\n",
    "\n",
    "sns_rating.set(title = 'Top 20 drugs with 1/10 rating', ylabel = 'Number of Ratings', xlabel = \"Drug Names\")\n",
    "\n",
    "plt.setp(sns_rating.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea235099",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# A countplot of the ratings so we can see the distribution of the ratings\n",
    "plt.rcParams['figure.figsize'] = [20,8]\n",
    "sns.set(font_scale = 1.4, style = 'whitegrid')\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "sns_1 = sns.countplot(data['rating'], palette = 'spring', order = list(range(10, 0, -1)), ax = ax[0])\n",
    "sns_2 = sns.distplot(data['rating'], ax = ax[1])\n",
    "sns_1.set_title('Count of Ratings')\n",
    "sns_1.set_xlabel(\"Rating\")\n",
    "\n",
    "sns_2.set_title('Distribution of Ratings')\n",
    "sns_2.set_xlabel(\"Rating\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c84b9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Word cloud of the reviews with rating equal to 10\n",
    "df_rate_ten = data.loc[data.rating == 10, 'review']\n",
    "k = (' '.join(df_rate_ten))\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(k)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5675f3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Word cloud of the reviews with rating equal to 1\n",
    "\n",
    "df_rate_one = data.loc[data.rating == 1, 'review']\n",
    "k1 = (' '.join(df_rate_one))\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate(k1)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41963f0f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This barplot shows the mean rating of the drugs per year\n",
    "\n",
    "mean_rating = dict(data.groupby(data['date'].dt.year)['rating'].mean())\n",
    "plt.rcParams['figure.figsize'] = [12, 7]\n",
    "sns.set(font_scale = 1.2, style = 'whitegrid')\n",
    "sns_ = sns.barplot(x = list(mean_rating.keys()), y = list(mean_rating.values()), color = 'orange');\n",
    "sns_.set_xlabel(\"Year\")\n",
    "sns_.set_ylabel(\"Rating\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb535719",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This barplot show the top 10 conditions the people are suffering.\n",
    "\n",
    "cond = dict(data['condition'].value_counts())\n",
    "top_condition = list(cond.keys())[0:10]\n",
    "values = list(cond.values())[0:10]\n",
    "sns.set(style = 'darkgrid', font_scale = 1.3)\n",
    "plt.rcParams['figure.figsize'] = [18, 7]\n",
    "\n",
    "sns_ = sns.barplot(x = top_condition, y = values, palette = 'winter')\n",
    "sns_.set_title(\"Top 10 conditions\")\n",
    "sns_.set_xlabel(\"Conditions\")\n",
    "sns_.set_ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7451d2b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 drugs which are used for the top condition, that is Birth Control\n",
    "\n",
    "df = data[data['condition'] == 'Birth Control']['drugname'].value_counts()[0: 10]\n",
    "sns.set(font_scale = 1.2, style = 'darkgrid')\n",
    "\n",
    "sns_ = sns.barplot(x = df.index, y = df.values, palette = 'summer')\n",
    "sns_.set_xlabel('Drug Names')\n",
    "sns_.set_title(\"Top 10 Drugs used for Birth Control\")\n",
    "plt.setp(sns_.get_xticklabels(), rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97033afe",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of the useful count\n",
    "sns.set(style = 'whitegrid', font_scale = 1.3)\n",
    "plt.rcParams['figure.figsize'] = [12, 7]\n",
    "sns.distplot(data['usefulcount'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53523f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This barplot shows the number of reviews per year\n",
    "df = data['date'].dt.year.value_counts()\n",
    "df = df.sort_index()\n",
    "\n",
    "sns_ = sns.barplot(x = df.index, y = df.values, color = 'mediumaquamarine')\n",
    "sns_.set_title(\"Number of reviews per year\")\n",
    "sns_.set_xlabel(\"Year\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0cc02",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap of the correlation matrix\n",
    "plt.rcParams['figure.figsize'] = [7,5]\n",
    "sns.set(font_scale = 1.2)\n",
    "corr = data.select_dtypes(include = 'int64').corr()\n",
    "sns_heat = sns.heatmap(corr, annot = True, vmin=-1, vmax=1, center=0,\n",
    "            cmap=sns.diverging_palette(20, 220, n=200), square=True);\n",
    "plt.setp(sns_heat.get_xticklabels(), rotation = 45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90c632",
   "metadata": {},
   "source": [
    "### Labels Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06ef34",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_bar_chart(df):\n",
    "    # analyze the condition labels\n",
    "    counts_series = df.condition.value_counts()\n",
    "    counts_df = pd.DataFrame(counts_series)\n",
    "    counts_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "    number_of_classes(df)\n",
    "\n",
    "    fig = px.bar(counts_df, x=\"index\", y=\"condition\", orientation='v',\n",
    "              height=400,\n",
    "              title='xc')\n",
    "    fig.show()\n",
    "\n",
    "def number_of_classes(df):\n",
    "    print(\"Number of classes: \", len(df[\"condition\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ea486",
   "metadata": {},
   "source": [
    "### 4.Class Distribution (Here class is conditions and that particular condition have more than 20 reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b7b5a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Keeping classes which have more than 20 values in them\n",
    "index_counts = df_train[\"condition\"].value_counts()[df_train.condition.value_counts() >= 20].index\n",
    "df_train = df_train[df_train[\"condition\"].isin(index_counts)]\n",
    "\n",
    "number_of_classes(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e6d49",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_bar_chart(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea8f4b",
   "metadata": {},
   "source": [
    "### 5. Undersampling classes with more than 200 samples ((only 200 reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128666a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# undersampling all classes with samples greater than 200 to 200\n",
    "condition_over200 = df_train[\"condition\"].value_counts()[df_train.condition.value_counts() >= 200].index\n",
    "\n",
    "for condition in condition_over200:\n",
    "    # randomly shuffle the samples\n",
    "    condition_samples = df_train[df_train[\"condition\"]==condition]\n",
    "    condition_samples = condition_samples.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # extract only 200\n",
    "    condition_samples = condition_samples[:200]\n",
    "\n",
    "    df_train = df_train[df_train[\"condition\"]!=condition]\n",
    "    # put it back\n",
    "    df_train = pd.concat([df_train, condition_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d6282",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_bar_chart(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06266c50",
   "metadata": {},
   "source": [
    "### 6. Filtering Labels and removing alpha numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9307f98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels):\n",
    "    labels = labels.tolist()\n",
    "    labels_truth = []\n",
    "    for label in labels:\n",
    "        #print(label)\n",
    "        if label[0].isdigit():\n",
    "            labels_truth.append(False)\n",
    "        else:\n",
    "            labels_truth.append(True)\n",
    "    return labels_truth\n",
    "\n",
    "df_train = df_train[filter_labels(df_train[\"condition\"])]\n",
    "print(\"Train \", number_of_classes(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae727b4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_bar_chart(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b6ddb",
   "metadata": {},
   "source": [
    "### 7.Dropping all NA values of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e642d51",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_test.dropna(how = 'any', axis = 0, inplace = True)\n",
    "df_test = df_test[filter_labels(df_test[\"condition\"])]\n",
    "print(\"Test \", number_of_classes(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fba64",
   "metadata": {},
   "source": [
    "### 8. Only keeping the classes that are in the training set, in to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521be770",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_test[df_test[\"condition\"].isin(df_train[\"condition\"])]\n",
    "number_of_classes(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9ee9e",
   "metadata": {},
   "source": [
    "### 9.Using stopwords to focus on important words. Using stemmer to focus on the standard form of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812dd5e",
   "metadata": {},
   "source": [
    "#### NLP Preprocessing on reviews: \n",
    "##### Removing stopwords and punctuations\n",
    "##### Stemming using Snowball Stemmer algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17c566",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "nltk.download('stopwords')\n",
    "def filter_data(reviews):\n",
    "  \n",
    "  \"\"\"\n",
    "  Filter the corpus of training and testing df.\n",
    "  This function removes stop and stem words from the corpus\n",
    "  :param reviews:\n",
    "  :return:\n",
    "  \"\"\"\n",
    "  stop = stopwords.words('english')\n",
    "  stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "  # remove punctuations\n",
    "  series = reviews.str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "  # remove stop words\n",
    "  series = series.apply(\n",
    "      lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "\n",
    "  # remove stem words\n",
    "  series = series.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "  return series\n",
    "    \n",
    "df_train[\"review\"] = filter_data(df_train[\"review\"]).str.lower()\n",
    "df_test[\"review\"] = filter_data(df_test[\"review\"]).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be0153",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc1207",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# shuffle the training dataframe and saving the columns in X and Y\n",
    "df_train[\"Label\"] = df_train[\"condition\"].str.lower()\n",
    "df_test[\"Label\"] = df_test[\"condition\"].str.lower()\n",
    "\n",
    "df_train = df_train.sample(frac=1)\n",
    "X = df_train['review']\n",
    "Y = df_train['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982bc599",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_count_vec = count_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf55fd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(#max_df = 0.75,\n",
    "                                   max_features = 500\n",
    "                                   )\n",
    "# we can combine fit and transform steps into a single step using fit_transform()\n",
    "X_count_vec = tfidf_vectorizer.fit_transform(X)\n",
    "print(f'tfidf vectors in array (dense) format\\n')\n",
    "print(X_count_vec.toarray())\n",
    "print(f'\\nThe shape of the tfidf vectors is : {X_count_vec.toarray().shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf48fd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb7d1d",
   "metadata": {},
   "source": [
    "#### 4-fold Cross-Validation on ML Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708e3d8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db1d7b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def cross_val_multiple_classifiers(X, Y):\n",
    "    classifiers = [#MultinomialNB(), SGDClassifier(loss=\"modified_huber\", n_jobs = -1),\n",
    "                 RandomForestClassifier(n_estimators=100, random_state = 21, verbose =1, n_jobs = 2),\n",
    "                 LogisticRegression(random_state = 21, max_iter = 100, C = 10, n_jobs = 2)]\n",
    "    #classifiers = [XGBClassifier(n_estimators=200, subsample = 0.7, colsample_bytree = 0.75, random_state = 21 )]\n",
    "    labels = [#'Multinomial Naive Bayes', 'SGD Classifier',\n",
    "              'Random Forest',  'Logistic Regression']\n",
    "    #labels = ['XGBClassifier']\n",
    "    clf_cv_mean = []\n",
    "    clf_cv_std = []\n",
    "    for clf, label in zip(classifiers, labels):\n",
    "        print(label)\n",
    "        scores = cross_val_score(clf, X, Y, cv=4, scoring='accuracy')\n",
    "        print (\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "\n",
    "# calling multiple classifiers on the vectorized features\n",
    "cross_val_multiple_classifiers(X_count_vec, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4de4d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_count_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b4a6f",
   "metadata": {},
   "source": [
    "### 10.Training the best model for getting results on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6bb9f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_hyper = dict([#('max_depth', 170), \n",
    " ('max_features', 0.85308425024733383), \n",
    "  ('max_samples', 0.8810228445606806), \n",
    "   #('min_samples_split', 0.13968408086154427), \n",
    "                   ('verbose', 1), \n",
    "                   ('random_state', 21), \n",
    "                   ('n_jobs', 8),\n",
    "    ('n_estimators', 947)])\n",
    "best_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a6757",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# training the Random Forest Classifier on complete training data\n",
    "fin_clf = RandomForestClassifier(**best_hyper)\n",
    "\n",
    "fin_clf.fit(X_count_vec, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfea6e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# getting preds on the test data\n",
    "preds = fin_clf.predict(X_count_vec)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "print(\"Accuracy on train data: \", accuracy_score(df_train[\"Label\"].str.lower(),\n",
    "                                                preds))\n",
    "print(\"Macro F-1 Score on train data: \", f1_score(df_train[\"Label\"].str.lower(),\n",
    "                                                preds, average=\"macro\"))\n",
    "print(\"Micro F-1 Score on train data: \", f1_score(df_train[\"Label\"].str.lower(),\n",
    "                                                preds, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecbe2d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# transforming test_data with count vectorizer\n",
    "X_test_vec = tfidf_vectorizer.transform(df_test['review'])\n",
    "\n",
    "# getting preds on the test data\n",
    "preds = fin_clf.predict(X_test_vec)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "print(\"Accuracy on test data: \", accuracy_score(df_test[\"Label\"].str.lower(),\n",
    "                                                preds))\n",
    "print(\"Macro F-1 Score on test data: \", f1_score(df_test[\"Label\"].str.lower(),\n",
    "                                                preds, average=\"macro\"))\n",
    "print(\"Micro F-1 Score on test data: \", f1_score(df_test[\"Label\"].str.lower(),\n",
    "                                                preds, average=\"micro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122cb663",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, precision_score\n",
    "print(\"Accuracy on test data: \", precision_recall_fscore_support(df_test[\"Label\"].str.lower(), \n",
    "                                                preds))\n",
    "print(\"Macro F-1 Score on test data: \", precision_score(df_test[\"Label\"].str.lower(), \n",
    "                                                preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583a924",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "#### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600363fc",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56a280",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers -U -qq\n",
    "!pip install sentencepiece -U -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b18f0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade jax jaxlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd35cfa",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jaxlib\n",
    "\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"JAXLib version:\", jaxlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23244f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# MPS acceleration is available on MacOS 12.3+\n",
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894c273",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    cudf-cu12 dask-cudf-cu12 cuml-cu12 cugraph-cu12 cuspatial-cu12 cuproj-cu12 cuxfilter-cu12 cucim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0e5fa",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install cudf-cu11 dask-cudf-cu11 --extra-index-url=https://pypi.nvidia.com\n",
    "!pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com\n",
    "!pip install cugraph-cu11 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb5009",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import the pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import the numpy library for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Import the Matplotlib library for creating visualizations such as plots, graphs, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the pathlib library for working with file paths in a way that is cross-platform\n",
    "from pathlib import Path\n",
    "\n",
    "# Import functions for metrics computation like confusion matrix, and accuracy\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "\n",
    "# Import the transformers library for state-of-the-art Natural Language Processing (NLP) models like BERT, GPT, etc.\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1a742",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  base_folder = Path('/content/drive/MyDrive/NLP') # MAKE SURE TO CHANGE THE PATH\n",
    "\n",
    "# (the else block is required only if you have local GPU machine, other wise you can ignore the else block)\n",
    "else:\n",
    "  base_folder = Path('/Users/shritej/Documents/GitHub/archive') # MAKE SURE TO CHANGE THE PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57523b2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import the torch library. PyTorch is a Python library for deep learning.\n",
    "import torch\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available for PyTorch.\n",
    "# This can speed up neural network computations.\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd9841",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}